{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24193,
     "status": "ok",
     "timestamp": 1737504787894,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "BD6mAxOQWdQN",
    "outputId": "4d63b869-38d6-430c-8fa2-846e4f64325b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kiwipiepy\n",
      "  Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Collecting kiwipiepy_model<0.21,>=0.20 (from kiwipiepy)\n",
      "  Downloading kiwipiepy_model-0.20.0.tar.gz (34.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kiwipiepy_model\n",
      "  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.20.0-py3-none-any.whl size=34818026 sha256=218817eb3878924669acc91d7088ddd73cd1f6e9c938967564ba0239be81d0f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/c8/52/3a539d6e9065b191fe1c215e0203dcc3e00601c0e3d3d39824\n",
      "Successfully built kiwipiepy_model\n",
      "Installing collected packages: kiwipiepy_model, kiwipiepy\n",
      "Successfully installed kiwipiepy-0.20.3 kiwipiepy_model-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kiwipiepy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGyymIQyWq7K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import kiwipiepy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoAp53s9eL_-"
   },
   "source": [
    "# 형태소 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3112,
     "status": "ok",
     "timestamp": 1737513943843,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "SFp6MVYseEiP",
    "outputId": "0fcca943-2e2a-4d37-f4b6-9dc9e86d8fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 완료.\n",
      "사용자 사전 단어 수: 232개\n",
      "사용자 사전 단어 등록 완료.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "import os\n",
    "\n",
    "# 1. CSV 파일 경로 설정 (실제 파일 경로로 수정하세요)\n",
    "merged_df_file = '/content/2025-01-21-12_final_lower.csv'          # 입력 CSV 파일 경로\n",
    "user_dictionary_file = '/content/user_dict_lower.txt'   # 사용자 사전 파일 경로\n",
    "output_file = '/content/[lower]analyzed_job_data_final.csv'            # 출력 CSV 파일 경로\n",
    "\n",
    "# 2. CSV 파일 읽기\n",
    "try:\n",
    "    merged_df = pd.read_csv(merged_df_file)\n",
    "    print(\"CSV 파일 읽기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 파일을 찾을 수 없습니다. 경로를 확인하세요: {merged_df_file}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. Kiwi 초기화 및 사용자 사전 적용\n",
    "# kiwi = Kiwi(typos='basic_with_continual_and_lengthening')  # 기본 오타 정보, 연철, 장음화 함께 사용\n",
    "kiwi = Kiwi(typos='basic_with_continual')\n",
    "\n",
    "# 3.1. 사용자 사전 파일에서 단어 읽기\n",
    "try:\n",
    "    with open(user_dictionary_file, 'r', encoding='utf-8') as f:\n",
    "        user_custom_words = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"사용자 사전 단어 수: {len(user_custom_words)}개\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 사용자 사전 파일을 찾을 수 없습니다. 경로를 확인하세요: {user_dictionary_file}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3.2. 사용자 사전 단어를 NNP 품사로 등록\n",
    "for word in user_custom_words:\n",
    "    kiwi.add_user_word(word, 'SL')  # IT 용어는 보통 고유 명사(NNP)로 분류\n",
    "\n",
    "print(\"사용자 사전 단어 등록 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0oloTEo0a8a"
   },
   "outputs": [],
   "source": [
    "# 5. 형태소 분석 함수 정의\n",
    "allowed_pos_tags = ['SL']  # 외래어만 추출\n",
    "\n",
    "def analyze_foreign_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
    "    tokens = kiwi.tokenize(text, normalize_coda=True, split_complex=True)\n",
    "    # 외래어(SL)만 추출\n",
    "    foreign_words = [morph for morph, pos, _, _ in tokens if pos == 'SL']\n",
    "    # 중복된 토큰 제거 (순서 유지)\n",
    "    seen = set()\n",
    "    unique_foreign_words = []\n",
    "    for morph in foreign_words:\n",
    "        if morph not in seen:\n",
    "            seen.add(morph)\n",
    "            unique_foreign_words.append(morph)\n",
    "    return unique_foreign_words\n",
    "\n",
    "def analyze_foreign_words_all(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
    "    tokens = kiwi.tokenize(text, normalize_coda=True, split_complex=True)\n",
    "    combined_tokens = []\n",
    "    morph_seen = set()\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        morph, pos, _, _ = tokens[i]\n",
    "        # 현재 토큰이 SN이고 다음 토큰이 NNB인 경우 결합 (외래어 포함)\n",
    "        if pos == 'SN' and (i + 1) < len(tokens):\n",
    "            next_morph, next_pos, _, _ = tokens[i + 1]\n",
    "            if next_pos == 'NNB':\n",
    "                combined_morph = morph + next_morph\n",
    "                combined_pos = 'SN+NNB'\n",
    "                if pos == 'SL' and combined_morph not in morph_seen:\n",
    "                    combined_tokens.append(f\"{combined_morph}+{combined_pos}\")\n",
    "                    morph_seen.add(combined_morph)\n",
    "                i += 2\n",
    "                continue\n",
    "        # 외래어(SL)만 처리\n",
    "        if pos == 'SL' and morph not in morph_seen:\n",
    "            combined_tokens.append(f\"{morph}+{pos}\")\n",
    "            morph_seen.add(morph)\n",
    "        i += 1\n",
    "    return combined_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392833,
     "status": "ok",
     "timestamp": 1737514556327,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "TiJu1vmX1D-G",
    "outputId": "e1c7ca35-2925-480e-e42c-67c655608348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 시작...\n",
      "형태소 분석 완료.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
    "required_columns = ['description', 'requirement', 'preferredExperience']\n",
    "for col in required_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = ''\n",
    "\n",
    "# 7. 'description'과 'requirement'를 합쳐 새로운 열 생성\n",
    "merged_df['description_requirement'] = (\n",
    "    merged_df['description'].fillna('') + ' ' +\n",
    "    merged_df['requirement'].fillna('')\n",
    ")\n",
    "\n",
    "# 8. 'preferredExperience'는 별도로 새로운 열 생성 (필요시 다른 전처리 추가 가능)\n",
    "merged_df['preferredExperience_cleaned'] = merged_df['preferredExperience'].fillna('')\n",
    "\n",
    "# 9. 형태소 분석 적용\n",
    "print(\"형태소 분석 시작...\")\n",
    "\n",
    "# 분석 함수 호출 시 사용될 함수명 수정\n",
    "merged_df['combined_analyzed'] = merged_df['description_requirement'].apply(analyze_foreign_words)\n",
    "merged_df['combined_analyzed_all'] = merged_df['description_requirement'].apply(analyze_foreign_words_all)\n",
    "merged_df['preferredExperience_analyzed'] = merged_df['preferredExperience_cleaned'].apply(analyze_foreign_words)\n",
    "merged_df['preferredExperience_analyzed_all'] = merged_df['preferredExperience_cleaned'].apply(analyze_foreign_words_all)\n",
    "\n",
    "print(\"형태소 분석 완료.\")\n",
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1737514560943,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "0MBbJMjx0iAF",
    "outputId": "4d8260de-b04b-4c50-a982-921a8a7fa7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과가 '/content/[lower]analyzed_job_data_final.csv' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 7.5. 결과를 새로운 CSV 파일에 저장\n",
    "output_columns = [\n",
    "    'id',\n",
    "    'description',\n",
    "    'requirement',\n",
    "    'preferredExperience',\n",
    "    'description_requirement',  # 'description+requirement' -> 'description_requirement'으로 변경\n",
    "    'combined_analyzed',  # 'description_requirement_analyzed' -> 'combined_analyzed'으로 변경\n",
    "    'combined_analyzed_all',  # 'description_requirement_analyzed_all' -> 'combined_analyzed_all'으로 변경\n",
    "    'preferredExperience_analyzed',\n",
    "    'preferredExperience_analyzed_all'\n",
    "]\n",
    "# 필요한 열만 선택하여 저장 (존재하지 않는 열은 제외)\n",
    "existing_output_columns = [col for col in output_columns if col in merged_df.columns]\n",
    "\n",
    "# 출력 디렉토리가 존재하지 않으면 생성\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "merged_df[existing_output_columns].to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"형태소 분석 결과가 '{output_file}' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGtLVU0f_ccI"
   },
   "source": [
    "# 형태소 분석 결과, 메타 DB로 항목화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1737515876605,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "5TKUBhh0BBp9",
    "outputId": "4351027f-317b-4407-9c9a-e3a4cf46de2f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/[lower]separate_json_columns_data.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert the 'combined_analyzed' and 'preferredExperience_analyzed' columns to JSON format separately\n",
    "df['combined_analyzed_json'] = df['combined_analyzed'].apply(lambda x: json.dumps(eval(x)) if isinstance(x, str) else json.dumps([]))\n",
    "df['preferredExperience_analyzed_json'] = df['preferredExperience_analyzed'].apply(lambda x: json.dumps(eval(x)) if isinstance(x, str) else json.dumps([]))\n",
    "\n",
    "# Save the result with the separate JSON columns\n",
    "output_file_path_json = '/content/[lower]separate_json_columns_data.csv'\n",
    "df[['id', 'combined_analyzed_json', 'preferredExperience_analyzed_json']].to_csv(output_file_path_json, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Provide the link to download the file\n",
    "output_file_path_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE-4Cn2lcy_I"
   },
   "source": [
    "# 코드너리 사전에 없는 기술들 추출한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1737523204169,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "ABtl-5AIc2fB",
    "outputId": "1477f570-58f3-4314-cb2d-3f798f96346c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/[lower_test1]filtered_non_matching_data.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the categories provided (converted to lowercase)\n",
    "languages = [\n",
    "    'c++', 'c#', 'dart', 'elixir', 'groovy', 'java', 'javascript', 'lua', 'perl', 'php', 'python', 'r',\n",
    "    'ruby', 'rust', 'scala', 'swift', 'typescript', 'rescript', 'go'\n",
    "]\n",
    "frameworks = [\n",
    "    'angular', 'appium', 'armeria', 'backbonejs', 'codeigniter', 'dagger', 'dropwizard', 'echo', 'electron',\n",
    "    'emberjs', 'expressjs', 'falcon', 'fastapi', 'fastify', 'fiber', 'flask', 'flink', 'gatsby', 'gin', 'grpc',\n",
    "    'hadoop', 'jasmine', 'junit', 'kotest', 'ktor', 'laravel', 'meteor', 'mocha', 'mockito', 'nestjs', 'netty',\n",
    "    'nextjs', 'nuxtjs', 'phoenix', 'ray', 'react native', 'reactorkit', 'relay', 'ribs', 'ruby on rails', 'sanic',\n",
    "    'selenium', 'spark', 'spring', 'springboot', 'svelte', 'swagger', 'tailwind', 'thrift', 'vuejs', 'nodejs', 'koa'\n",
    "]\n",
    "libraries = [\n",
    "    'alamofire', 'apollo', 'emotion', 'enzyme', 'exoplayer', 'glide', 'immer', 'jotai', 'keras', 'lottie', 'mobx',\n",
    "    'moya', 'react context', 'reactivex', 'reactjs', 'react query', 'recoil', 'redux', 'rest-assured', 'retrofit',\n",
    "    'sinon', 'snapkit', 'styled-components', 'tensorflow', 'testing library', 'vuex', 'zustand', 'pytorch', 'graphql'\n",
    "]\n",
    "tools = [\n",
    "    'agit', 'airflow', 'ansible', 'arcus', 'arangodb', 'argo cd', 'asana', 'aws athena', 'aws auroradb', 'aws codebuild',\n",
    "    'aws codedeploy', 'aws codepipeline', 'aws documentdb', 'aws dynamodb', 'aws kinesis', 'aws mariadb', 'aws redshift',\n",
    "    'aws ses', 'aws sns', 'azure devops', 'bazel', 'bitbucket', 'bitrise', 'capistrano', 'cassandradb', 'celery', 'central dogma',\n",
    "    'ceph', 'circle ci', 'clickhouse', 'cockroachdb', 'confluence', 'couchbase', 'cubrid', 'cucumber', 'cypress', 'discord', 'docker',\n",
    "    'docusaurus', 'dooray', 'drone', 'druid', 'elasticsearch', 'fastlane', 'fluentd', 'flow', 'github', 'github action', 'gitlab',\n",
    "    'go cd', 'google bigquery', 'google cloud build', 'google data studio', 'google firebase', 'grafana', 'greenplum', 'gulp', 'h2',\n",
    "    'harbor', 'hbase', 'hazelcast', 'helm', 'hive', 'hugo', 'istio', 'jaeger', 'jandi', 'jenkins', 'jira', 'kafka', 'kakaotalk',\n",
    "    'kakaowork', 'karma', 'karpenter', 'kibana', 'kube-bench', 'kubeflow', 'kubernetes', 'kudu', 'linkerd', 'liquibase', 'locust',\n",
    "    'looker', 'memcached', 'metabase', 'microsoft-teams', 'mlflow', 'monday', 'mongodb', 'mssql', 'mysql', 'naver works', 'neo4j',\n",
    "    'nexus', 'ngrinder', 'nifi', 'notion', 'openebs', 'oracledb', 'packer', 'playwright', 'postgresql', 'presto', 'prometheus',\n",
    "    'rabbitmq', 'rancher', 'ranger', 'redash', 'redis', 'rocksdb', 'saltstack', 'slack', 'snowflake', 'solr', 'sonarqube',\n",
    "    'storybook', 'superset', 'tableau', 'telegram', 'travis ci', 'trello', 'trino', 'tuist', 'unity', 'vault', 'zabbix', 'zeppelin',\n",
    "    'playwright', 'puppeteer', 'opengl', 'zipkin', 'impala', 'influxdb', 'traefik'\n",
    "]\n",
    "\n",
    "# Load the uploaded file to check its contents\n",
    "file_path = '/content/[lower]separate_json_columns_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to filter out items that are in the categories\n",
    "def filter_non_matching_items(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = eval(tokens)  # Convert string to list\n",
    "    non_matching_items = [item for item in tokens if item.lower() not in\n",
    "                          (languages + frameworks + libraries + tools)]\n",
    "    return non_matching_items\n",
    "\n",
    "# Apply filtering to both columns\n",
    "df['combined_analyzed_non_matching'] = df['combined_analyzed_json'].apply(filter_non_matching_items)\n",
    "df['preferredExperience_analyzed_non_matching'] = df['preferredExperience_analyzed_json'].apply(filter_non_matching_items)\n",
    "\n",
    "# Save the filtered result to a new file\n",
    "output_file_path_filtered = '/content/[lower_test1]filtered_non_matching_data.csv'\n",
    "df[['id', 'combined_analyzed_non_matching', 'preferredExperience_analyzed_non_matching']].to_csv(output_file_path_filtered, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Provide the link to download the file\n",
    "output_file_path_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jns07T-ojn9P"
   },
   "source": [
    "# C를 제외한 1음절 단어 모두 제거한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg1FG-O6jqO9"
   },
   "outputs": [],
   "source": [
    "# /content/[lower_test1]1음절_filtered_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2325,
     "status": "ok",
     "timestamp": 1737602334307,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "_ZKyYEbIJXed",
    "outputId": "97843273-d672-4f80-8b72-0410d85fcab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동의어 사전이 생성되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 파일 경로 및 컬럼 이름\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "combined_col = 'combined_analyzed_filtered'\n",
    "preferred_col = 'preferredExperience_analyzed_filtered'\n",
    "\n",
    "# 대표값 리스트\n",
    "categories = {\n",
    "    \"languages\": [\n",
    "        'c++', 'c#', 'dart', 'elixir', 'groovy', 'java', 'javascript', 'lua', 'perl', 'php', 'python', 'r',\n",
    "        'ruby', 'rust', 'scala', 'swift', 'typescript', 'rescript', 'go'\n",
    "    ],\n",
    "    \"frameworks\": [\n",
    "        'angular', 'appium', 'armeria', 'backbonejs', 'codeigniter', 'dagger', 'dropwizard', 'echo', 'electron',\n",
    "        'emberjs', 'expressjs', 'falcon', 'fastapi', 'fastify', 'fiber', 'flask', 'flink', 'gatsby', 'gin', 'grpc',\n",
    "        'hadoop', 'jasmine', 'junit', 'kotest', 'ktor', 'laravel', 'meteor', 'mocha', 'mockito', 'nestjs', 'netty',\n",
    "        'nextjs', 'nuxtjs', 'phoenix', 'ray', 'react native', 'reactorkit', 'relay', 'ribs', 'ruby on rails', 'sanic',\n",
    "        'selenium', 'spark', 'spring', 'springboot', 'svelte', 'swagger', 'tailwind', 'thrift', 'vuejs', 'nodejs', 'koa'\n",
    "    ],\n",
    "    \"libraries\": [\n",
    "        'alamofire', 'apollo', 'emotion', 'enzyme', 'exoplayer', 'glide', 'immer', 'jotai', 'keras', 'lottie', 'mobx',\n",
    "        'moya', 'react context', 'reactivex', 'reactjs', 'react query', 'recoil', 'redux', 'rest-assured', 'retrofit',\n",
    "        'sinon', 'snapkit', 'styled-components', 'tensorflow', 'testing library', 'vuex', 'zustand', 'pytorch', 'graphql'\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        'agit', 'airflow', 'ansible', 'arcus', 'arangodb', 'argo cd', 'asana', 'aws athena', 'aws auroradb', 'aws codebuild',\n",
    "        'aws codedeploy', 'aws codepipeline', 'aws documentdb', 'aws dynamodb', 'aws kinesis', 'aws mariadb', 'aws redshift',\n",
    "        'aws ses', 'aws sns', 'azure devops', 'bazel', 'bitbucket', 'bitrise', 'capistrano', 'cassandradb', 'celery', 'central dogma',\n",
    "        'ceph', 'circle ci', 'clickhouse', 'cockroachdb', 'confluence', 'couchbase', 'cubrid', 'cucumber', 'cypress', 'discord', 'docker',\n",
    "        'docusaurus', 'dooray', 'drone', 'druid', 'elasticsearch', 'fastlane', 'fluentd', 'flow', 'github', 'github action', 'gitlab',\n",
    "        'go cd', 'google bigquery', 'google cloud build', 'google data studio', 'google firebase', 'grafana', 'greenplum', 'gulp', 'h2',\n",
    "        'harbor', 'hbase', 'hazelcast', 'helm', 'hive', 'hugo', 'istio', 'jaeger', 'jandi', 'jenkins', 'jira', 'kafka', 'kakaotalk',\n",
    "        'kakaowork', 'karma', 'karpenter', 'kibana', 'kube-bench', 'kubeflow', 'kubernetes', 'kudu', 'linkerd', 'liquibase', 'locust',\n",
    "        'looker', 'memcached', 'metabase', 'microsoft-teams', 'mlflow', 'monday', 'mongodb', 'mssql', 'mysql', 'naver works', 'neo4j',\n",
    "        'nexus', 'ngrinder', 'nifi', 'notion', 'openebs', 'oracledb', 'packer', 'playwright', 'postgresql', 'presto', 'prometheus',\n",
    "        'rabbitmq', 'rancher', 'ranger', 'redash', 'redis', 'rocksdb', 'saltstack', 'slack', 'snowflake', 'solr', 'sonarqube',\n",
    "        'storybook', 'superset', 'tableau', 'telegram', 'travis ci', 'trello', 'trino', 'tuist', 'unity', 'vault', 'zabbix', 'zeppelin',\n",
    "        'playwright', 'puppeteer', 'opengl', 'zipkin', 'impala', 'influxdb', 'traefik'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 동의어 사전 생성 함수\n",
    "def create_synonym_dict():\n",
    "    df = pd.read_csv(file_path)\n",
    "    synonym_dict = {category: {} for category in categories.keys()}\n",
    "\n",
    "    for category, terms in categories.items():\n",
    "        for term in terms:\n",
    "            synonym_dict[category][term] = [term]  # 대표값 포함\n",
    "            for col in [combined_col, preferred_col]:\n",
    "                matches = df[col].dropna().apply(lambda x: term.replace(\" \", \"\").lower() in str(x).replace(\" \", \"\").lower())\n",
    "                matched_terms = df[col][matches].str.lower().str.strip().unique().tolist()\n",
    "                synonym_dict[category][term].extend([t for t in matched_terms if t != term])\n",
    "            synonym_dict[category][term] = list(set(synonym_dict[category][term]))  # 중복 제거\n",
    "\n",
    "    return synonym_dict\n",
    "\n",
    "# 동의어 사전 생성\n",
    "synonym_dict = create_synonym_dict()\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('/content/[gayoon2_final]synonym_dict.json', 'w') as f:\n",
    "    json.dump(synonym_dict, f, indent=2)\n",
    "\n",
    "print(\"동의어 사전이 생성되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWRmwTK2K_0k"
   },
   "outputs": [],
   "source": [
    "# 최종 : 내가 만든 동의어 사전은 final_synonym_corrected.json\n",
    "# 현아님과 취합한 최종 동의어 사전\n",
    "# updated_synonym_corrected.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1737602589450,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "x46Cc9mBLx3M",
    "outputId": "697cd2d3-0cb5-47c6-c6a3-9fa82f012d94"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/[lower_test1]1음절_filtered_data.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Re-import the file after environment reset\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the DataFrame to JSON\n",
    "json_data = df.to_json(orient='records', force_ascii=False, indent=2)\n",
    "\n",
    "# Save the JSON file\n",
    "output_path = '/content/[lower_test1]1음절_filtered_data.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3062,
     "status": "ok",
     "timestamp": 1737603392390,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "J5-tav8bO23i",
    "outputId": "078ac8d7-c946-4857-bc95-ca3ba6d28586"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/[FINAL_GY]synonym_dictionary.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to analyze\n",
    "combined_col = 'combined_analyzed_filtered'\n",
    "preferred_col = 'preferredExperience_analyzed_filtered'\n",
    "\n",
    "# Define the categories\n",
    "categories = {\n",
    "    \"languages\": [\n",
    "        'c++', 'c#', 'dart', 'elixir', 'groovy', 'java', 'javascript', 'lua', 'perl', 'php', 'python', 'r',\n",
    "        'ruby', 'rust', 'scala', 'swift', 'typescript', 'rescript', 'go'\n",
    "    ],\n",
    "    \"frameworks\": [\n",
    "        'angular', 'appium', 'armeria', 'backbonejs', 'codeigniter', 'dagger', 'dropwizard', 'echo', 'electron',\n",
    "        'emberjs', 'expressjs', 'falcon', 'fastapi', 'fastify', 'fiber', 'flask', 'flink', 'gatsby', 'gin', 'grpc',\n",
    "        'hadoop', 'jasmine', 'junit', 'kotest', 'ktor', 'laravel', 'meteor', 'mocha', 'mockito', 'nestjs', 'netty',\n",
    "        'nextjs', 'nuxtjs', 'phoenix', 'ray', 'react native', 'reactorkit', 'relay', 'ribs', 'ruby on rails', 'sanic',\n",
    "        'selenium', 'spark', 'spring', 'springboot', 'svelte', 'swagger', 'tailwind', 'thrift', 'vuejs', 'nodejs', 'koa'\n",
    "    ],\n",
    "    \"libraries\": [\n",
    "        'alamofire', 'apollo', 'emotion', 'enzyme', 'exoplayer', 'glide', 'immer', 'jotai', 'keras', 'lottie', 'mobx',\n",
    "        'moya', 'react context', 'reactivex', 'reactjs', 'react query', 'recoil', 'redux', 'rest-assured', 'retrofit',\n",
    "        'sinon', 'snapkit', 'styled-components', 'tensorflow', 'testing library', 'vuex', 'zustand', 'pytorch', 'graphql'\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        'agit', 'airflow', 'ansible', 'arcus', 'arangodb', 'argo cd', 'asana', 'aws athena', 'aws auroradb', 'aws codebuild',\n",
    "        'aws codedeploy', 'aws codepipeline', 'aws documentdb', 'aws dynamodb', 'aws kinesis', 'aws mariadb', 'aws redshift',\n",
    "        'aws ses', 'aws sns', 'azure devops', 'bazel', 'bitbucket', 'bitrise', 'capistrano', 'cassandradb', 'celery', 'central dogma',\n",
    "        'ceph', 'circle ci', 'clickhouse', 'cockroachdb', 'confluence', 'couchbase', 'cubrid', 'cucumber', 'cypress', 'discord', 'docker',\n",
    "        'docusaurus', 'dooray', 'drone', 'druid', 'elasticsearch', 'fastlane', 'fluentd', 'flow', 'github', 'github action', 'gitlab',\n",
    "        'go cd', 'google bigquery', 'google cloud build', 'google data studio', 'google firebase', 'grafana', 'greenplum', 'gulp', 'h2',\n",
    "        'harbor', 'hbase', 'hazelcast', 'helm', 'hive', 'hugo', 'istio', 'jaeger', 'jandi', 'jenkins', 'jira', 'kafka', 'kakaotalk',\n",
    "        'kakaowork', 'karma', 'karpenter', 'kibana', 'kube-bench', 'kubeflow', 'kubernetes', 'kudu', 'linkerd', 'liquibase', 'locust',\n",
    "        'looker', 'memcached', 'metabase', 'microsoft-teams', 'mlflow', 'monday', 'mongodb', 'mssql', 'mysql', 'naver works', 'neo4j',\n",
    "        'nexus', 'ngrinder', 'nifi', 'notion', 'openebs', 'oracledb', 'packer', 'playwright', 'postgresql', 'presto', 'prometheus',\n",
    "        'rabbitmq', 'rancher', 'ranger', 'redash', 'redis', 'rocksdb', 'saltstack', 'slack', 'snowflake', 'solr', 'sonarqube',\n",
    "        'storybook', 'superset', 'tableau', 'telegram', 'travis ci', 'trello', 'trino', 'tuist', 'unity', 'vault', 'zabbix', 'zeppelin',\n",
    "        'playwright', 'puppeteer', 'opengl', 'zipkin', 'impala', 'influxdb', 'traefik'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize the synonym dictionary\n",
    "synonym_dict = {category: {term: [] for term in terms} for category, terms in categories.items()}\n",
    "\n",
    "# Iterate through the DataFrame and find matches for each category\n",
    "for category, terms in categories.items():\n",
    "    for term in terms:\n",
    "        # Check for matches in combined_analyzed_filtered column\n",
    "        combined_matches = df[combined_col].str.contains(term, case=False, na=False)\n",
    "        synonym_dict[category][term].extend(df[combined_col][combined_matches].unique().tolist())\n",
    "\n",
    "        # Check for matches in preferredExperience_analyzed_filtered column\n",
    "        preferred_matches = df[preferred_col].str.contains(term, case=False, na=False)\n",
    "        synonym_dict[category][term].extend(df[preferred_col][preferred_matches].unique().tolist())\n",
    "\n",
    "        # Remove duplicates and keep only unique synonyms\n",
    "        synonym_dict[category][term] = list(set(synonym_dict[category][term]))\n",
    "\n",
    "# Save the synonym dictionary as a JSON file\n",
    "output_path = '/content/[FINAL_GY]synonym_dictionary.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(synonym_dict, json_file, indent=4)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agbLoyXFl1c0"
   },
   "outputs": [],
   "source": [
    "# 우리가 식별하지 못한 목록 (기존 값과 중복x)\n",
    "categories = {\n",
    "    \"languages\": ['python', 'java', 'javascript', 'c', 'c++', 'c#', 'r', 'ruby', 'swift', 'kotlin', 'typescript',\n",
    "                  'go', 'php', 'perl', 'scala', 'bash', 'lua', 'matlab', 'rust'],\n",
    "    \"frameworks\": ['django', 'flask', 'spring', 'springboot', 'express', 'react', 'angular', 'vue', 'nextjs', 'nuxtjs',\n",
    "                   'fastapi', 'grpc', 'spark', 'hadoop', 'nestjs', 'svelte', 'emberjs', 'rails', 'fiber', 'koa'],\n",
    "    \"libraries\": ['tensorflow', 'pytorch', 'scikit-learn', 'numpy', 'pandas', 'seaborn', 'matplotlib', 'keras',\n",
    "                  'huggingface', 'xgboost', 'lightgbm', 'grpcio', 'opencv', 'beautifulsoup', 'sqlalchemy', 'requests',\n",
    "                  'fastai', 'bokeh', 'plotly', 'transformers', 'pydantic'],\n",
    "    \"tools\": ['docker', 'kubernetes', 'ansible', 'terraform', 'jenkins', 'circleci', 'airflow', 'grafana', 'prometheus',\n",
    "              'zabbix', 'elasticsearch', 'mongodb', 'redis', 'postgresql', 'mysql', 'neo4j', 'clickhouse', 'rabbitmq',\n",
    "              'kafka', 'snowflake', 'superset', 'tableau', 'metabase', 'apache spark', 'hive', 'hbase', 'cassandra',\n",
    "              'helm', 'istio', 'jaeger', 'linkerd', 'dask', 'bigquery', 'aws', 'azure', 'gcp', 'openstack']\n",
    "}\n",
    "\n",
    "# 프레임워크 - 제공된 틀만 넣어서 실행\n",
    "# 라이브러리 - 자유롭게 사용할 함수를 조합해서 사용\n",
    "\n",
    "# 우리가 식별하지 못한 목록(중복제거)\n",
    "categories = {\n",
    "    \"languages\": ['bash', 'matlab', 'c', 'kotlin'],\n",
    "    \"frameworks\": ['react', 'express', 'django', 'rails', 'vue'],\n",
    "    \"libraries\": ['sqlalchemy', 'transformers', 'matplotlib', 'opencv', 'huggingface', 'lightgbm', 'pandas', 'pydantic',\n",
    "                  'beautifulsoup', 'plotly', 'xgboost', 'grpcio', 'numpy', 'scikit-learn', 'bokeh', 'seaborn',\n",
    "                  'requests', 'fastai'],\n",
    "    \"tools\": ['bigquery', 'terraform', 'azure', 'gcp', 'openstack', 'cassandra', 'apache spark', 'dask', 'aws',\n",
    "              'circleci']\n",
    "}\n",
    "\n",
    "\n",
    "# 우리가 가진 목록\n",
    "categories = {\n",
    "    \"languages\": ['c++', 'c#', 'dart', 'elixir', 'groovy', 'java', 'javascript', 'lua', 'perl', 'php', 'python', 'r',\n",
    "                  'ruby', 'rust', 'scala', 'swift', 'typescript', 'rescript', 'go'],\n",
    "    \"frameworks\": ['angular', 'appium', 'armeria', 'backbonejs', 'codeigniter', 'dagger', 'dropwizard', 'echo',\n",
    "                   'electron', 'emberjs', 'expressjs', 'falcon', 'fastapi', 'fastify', 'fiber', 'flask', 'flink',\n",
    "                   'gatsby', 'gin', 'grpc', 'hadoop', 'jasmine', 'junit', 'kotest', 'ktor', 'laravel', 'meteor',\n",
    "                   'mocha', 'mockito', 'nestjs', 'netty', 'nextjs', 'nuxtjs', 'phoenix', 'ray', 'react native',\n",
    "                   'reactorkit', 'relay', 'ribs', 'ruby on rails', 'sanic', 'selenium', 'spark', 'spring',\n",
    "                   'springboot', 'svelte', 'swagger', 'tailwind', 'thrift', 'vuejs', 'nodejs', 'koa'],\n",
    "    \"libraries\": ['alamofire', 'apollo', 'emotion', 'enzyme', 'exoplayer', 'glide', 'immer', 'jotai', 'keras', 'lottie',\n",
    "                  'mobx', 'moya', 'react context', 'reactivex', 'reactjs', 'react query', 'recoil', 'redux',\n",
    "                  'rest-assured', 'retrofit', 'sinon', 'snapkit', 'styled-components', 'tensorflow', 'testing library',\n",
    "                  'vuex', 'zustand', 'pytorch', 'graphql'],\n",
    "    \"tools\": ['agit', 'airflow', 'ansible', 'arcus', 'arangodb', 'argo cd', 'asana', 'aws athena', 'aws auroradb',\n",
    "              'aws codebuild', 'aws codedeploy', 'aws codepipeline', 'aws documentdb', 'aws dynamodb', 'aws kinesis',\n",
    "              'aws mariadb', 'aws redshift', 'aws ses', 'aws sns', 'azure devops', 'bazel', 'bitbucket', 'bitrise',\n",
    "              'capistrano', 'cassandradb', 'celery', 'central dogma', 'ceph', 'circle ci', 'clickhouse', 'cockroachdb',\n",
    "              'confluence', 'couchbase', 'cubrid', 'cucumber', 'cypress', 'discord', 'docker', 'docusaurus', 'dooray',\n",
    "              'drone', 'druid', 'elasticsearch', 'fastlane', 'fluentd', 'flow', 'github', 'github action', 'gitlab',\n",
    "              'go cd', 'google bigquery', 'google cloud build', 'google data studio', 'google firebase', 'grafana',\n",
    "              'greenplum', 'gulp', 'h2', 'harbor', 'hbase', 'hazelcast', 'helm', 'hive', 'hugo', 'istio', 'jaeger',\n",
    "              'jandi', 'jenkins', 'jira', 'kafka', 'kakaotalk', 'kakaowork', 'karma', 'karpenter', 'kibana',\n",
    "              'kube-bench', 'kubeflow', 'kubernetes', 'kudu', 'linkerd', 'liquibase', 'locust', 'looker', 'memcached',\n",
    "              'metabase', 'microsoft-teams', 'mlflow', 'monday', 'mongodb', 'mssql', 'mysql', 'naver works', 'neo4j',\n",
    "              'nexus', 'ngrinder', 'nifi', 'notion', 'openebs', 'oracledb', 'packer', 'playwright', 'postgresql',\n",
    "              'presto', 'prometheus', 'rabbitmq', 'rancher', 'ranger', 'redash', 'redis', 'rocksdb', 'saltstack',\n",
    "              'slack', 'snowflake', 'solr', 'sonarqube', 'storybook', 'superset', 'tableau', 'telegram', 'travis ci',\n",
    "              'trello', 'trino', 'tuist', 'unity', 'vault', 'zabbix', 'zeppelin', 'playwright', 'puppeteer', 'opengl',\n",
    "              'zipkin', 'impala', 'influxdb', 'traefik']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2955,
     "status": "ok",
     "timestamp": 1737603836759,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "eTrpok8rPfKX",
    "outputId": "4ac29325-1a3c-43d1-c9ff-e6801d8aaa06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym dictionary saved to /content/[0123_gy]final_synonym_corrected.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSV file\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns to search in\n",
    "combined_col = 'combined_analyzed_filtered'\n",
    "preferred_col = 'preferredExperience_analyzed_filtered'\n",
    "\n",
    "# Define representative categories\n",
    "categories = {\n",
    "    \"languages\": ['c++', 'c#', 'dart', 'elixir', 'groovy', 'java', 'javascript', 'lua', 'perl', 'php', 'python', 'r',\n",
    "                  'ruby', 'rust', 'scala', 'swift', 'typescript', 'rescript', 'go'],\n",
    "    \"frameworks\": ['angular', 'appium', 'armeria', 'backbonejs', 'codeigniter', 'dagger', 'dropwizard', 'echo',\n",
    "                   'electron', 'emberjs', 'expressjs', 'falcon', 'fastapi', 'fastify', 'fiber', 'flask', 'flink',\n",
    "                   'gatsby', 'gin', 'grpc', 'hadoop', 'jasmine', 'junit', 'kotest', 'ktor', 'laravel', 'meteor',\n",
    "                   'mocha', 'mockito', 'nestjs', 'netty', 'nextjs', 'nuxtjs', 'phoenix', 'ray', 'react native',\n",
    "                   'reactorkit', 'relay', 'ribs', 'ruby on rails', 'sanic', 'selenium', 'spark', 'spring',\n",
    "                   'springboot', 'svelte', 'swagger', 'tailwind', 'thrift', 'vuejs', 'nodejs', 'koa'],\n",
    "    \"libraries\": ['alamofire', 'apollo', 'emotion', 'enzyme', 'exoplayer', 'glide', 'immer', 'jotai', 'keras', 'lottie',\n",
    "                  'mobx', 'moya', 'react context', 'reactivex', 'reactjs', 'react query', 'recoil', 'redux',\n",
    "                  'rest-assured', 'retrofit', 'sinon', 'snapkit', 'styled-components', 'tensorflow', 'testing library',\n",
    "                  'vuex', 'zustand', 'pytorch', 'graphql'],\n",
    "    \"tools\": ['agit', 'airflow', 'ansible', 'arcus', 'arangodb', 'argo cd', 'asana', 'aws athena', 'aws auroradb',\n",
    "              'aws codebuild', 'aws codedeploy', 'aws codepipeline', 'aws documentdb', 'aws dynamodb', 'aws kinesis',\n",
    "              'aws mariadb', 'aws redshift', 'aws ses', 'aws sns', 'azure devops', 'bazel', 'bitbucket', 'bitrise',\n",
    "              'capistrano', 'cassandradb', 'celery', 'central dogma', 'ceph', 'circle ci', 'clickhouse', 'cockroachdb',\n",
    "              'confluence', 'couchbase', 'cubrid', 'cucumber', 'cypress', 'discord', 'docker', 'docusaurus', 'dooray',\n",
    "              'drone', 'druid', 'elasticsearch', 'fastlane', 'fluentd', 'flow', 'github', 'github action', 'gitlab',\n",
    "              'go cd', 'google bigquery', 'google cloud build', 'google data studio', 'google firebase', 'grafana',\n",
    "              'greenplum', 'gulp', 'h2', 'harbor', 'hbase', 'hazelcast', 'helm', 'hive', 'hugo', 'istio', 'jaeger',\n",
    "              'jandi', 'jenkins', 'jira', 'kafka', 'kakaotalk', 'kakaowork', 'karma', 'karpenter', 'kibana',\n",
    "              'kube-bench', 'kubeflow', 'kubernetes', 'kudu', 'linkerd', 'liquibase', 'locust', 'looker', 'memcached',\n",
    "              'metabase', 'microsoft-teams', 'mlflow', 'monday', 'mongodb', 'mssql', 'mysql', 'naver works', 'neo4j',\n",
    "              'nexus', 'ngrinder', 'nifi', 'notion', 'openebs', 'oracledb', 'packer', 'playwright', 'postgresql',\n",
    "              'presto', 'prometheus', 'rabbitmq', 'rancher', 'ranger', 'redash', 'redis', 'rocksdb', 'saltstack',\n",
    "              'slack', 'snowflake', 'solr', 'sonarqube', 'storybook', 'superset', 'tableau', 'telegram', 'travis ci',\n",
    "              'trello', 'trino', 'tuist', 'unity', 'vault', 'zabbix', 'zeppelin', 'playwright', 'puppeteer', 'opengl',\n",
    "              'zipkin', 'impala', 'influxdb', 'traefik']\n",
    "}\n",
    "\n",
    "# Initialize final result dictionary\n",
    "final_synonyms = {key: {term: [] for term in terms} for key, terms in categories.items()}\n",
    "\n",
    "# Process the data\n",
    "for key, terms in categories.items():\n",
    "    for term in terms:\n",
    "        for col in [combined_col, preferred_col]:\n",
    "            matches = df[col].dropna().str.contains(term, case=False, na=False)\n",
    "            similar_items = df[col][matches].unique()\n",
    "            final_synonyms[key][term].extend(item.lower() for item in similar_items if item.lower() != term)\n",
    "\n",
    "# Remove duplicates\n",
    "for key in final_synonyms:\n",
    "    for term in final_synonyms[key]:\n",
    "        final_synonyms[key][term] = list(set(final_synonyms[key][term]))\n",
    "\n",
    "# Save to JSON\n",
    "output_path = '/content/[0123_gy]final_synonym_corrected.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(final_synonyms, json_file, indent=4)\n",
    "\n",
    "print(f\"Synonym dictionary saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1737605817937,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "sntE_mx4WnTV",
    "outputId": "16599910-a80f-4644-9512-91982a9b65cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거된 파일이 저장되었습니다: /content/[lower_test1]중복제거_filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "output_path = '/content/[lower_test1]중복제거_filtered_data.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 중복 제거\n",
    "df['preferredExperience_analyzed_filtered'] = df['preferredExperience_analyzed_filtered'].drop_duplicates().reset_index(drop=True)\n",
    "df['combined_analyzed_filtered'] = df['combined_analyzed_filtered'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 파일 저장\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"중복 제거된 파일이 저장되었습니다: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMQkO-pKZhIG"
   },
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "output_path = '/content/[lower_test1]중복제거2_filtered_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1737606180854,
     "user": {
      "displayName": "GAYOON CHOI",
      "userId": "14384576936901363882"
     },
     "user_tz": -540
    },
    "id": "PiGVKC_YZfN6",
    "outputId": "730f4caa-b310-46d1-a076-f6a4a6ce5dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거된 파일이 저장되었습니다: /content/[lower_test1]_deduplicated_filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 열 단위로 중복 제거 함수\n",
    "def remove_duplicates_per_column(df, columns):\n",
    "    \"\"\"\n",
    "    DataFrame의 특정 열에서 중복된 값을 제거합니다.\n",
    "    각 열의 중복을 독립적으로 처리합니다.\n",
    "    :param df: pandas DataFrame\n",
    "    :param columns: 중복 제거를 수행할 열 이름 목록\n",
    "    :return: 중복 제거된 DataFrame\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # 각 열에서 중복 제거\n",
    "        df[column] = df[column].apply(lambda x: ','.join(set(str(x).split(','))) if pd.notnull(x) else x)\n",
    "    return df\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = '/content/[lower_test1]1음절_filtered_data.csv'\n",
    "output_path = '/content/[lower_test1]_deduplicated_filtered_data.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 중복 제거 대상 열\n",
    "columns_to_deduplicate = ['preferredExperience_analyzed_filtered', 'combined_analyzed_filtered']\n",
    "\n",
    "# 열 단위 중복 제거\n",
    "df = remove_duplicates_per_column(df, columns_to_deduplicate)\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"중복 제거된 파일이 저장되었습니다: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItoNg3Milgm9"
   },
   "source": [
    "# 동의어 사전(기술 사전)에 2개의 칼럼에 값들이 있는지 확인하고, 있다면 메타DB 항목으로 넣어주기(칼럼)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3I0b44alnD5"
   },
   "outputs": [],
   "source": [
    "*# Google Colab에서 실행할 Python 코드\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON 데이터 경로 및 CSV 데이터 경로\n",
    "json_file_path = '/content/updated_synonym_corrected_final.json'\n",
    "csv_file_path = '//content/[lower_test1]1음절_filtered_data.csv'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(json_file_path, 'r') as file:\n",
    "    synonym_data = json.load(file)\n",
    "\n",
    "# CSV 데이터 로드\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 분석할 컬럼\n",
    "combined_column = 'combined_analyzed_filtered'\n",
    "preferred_column = 'preferredExperience_analyzed_filtered'\n",
    "\n",
    "# JSON 데이터 기반으로 카테고리를 찾는 함수\n",
    "def find_category(term, categories):\n",
    "    for category, items in categories.items():\n",
    "        for key, synonyms in items.items():\n",
    "            if term.lower() in [syn.lower() for syn in synonyms]:\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "# 새로운 컬럼 생성\n",
    "csv_data['combined_category'] = csv_data[combined_column].apply(lambda x: find_category(str(x), synonym_data))\n",
    "csv_data['preferred_category'] = csv_data[preferred_column].apply(lambda x: find_category(str(x), synonym_data))\n",
    "\n",
    "# 결과를 새 CSV 파일로 저장\n",
    "output_file_path = '/content/updated_filtered_with_categories.csv'\n",
    "csv_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"파일이 저장되었습니다: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsyb5NLd6rEu"
   },
   "source": [
    "# 아래는 무시 : 분석 속도 향상을 위한 청크 크기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUXy3lvr0bgv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. 청크 크기 설정\n",
    "chunksize = 10000  # 필요에 따라 조정\n",
    "\n",
    "# 6. 출력 파일 초기화 (헤더 포함)\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# 7. CSV 파일 청크 단위로 읽기 및 처리\n",
    "for chunk_number, chunk in enumerate(pd.read_csv(merged_df_file, chunksize=chunksize), start=1):\n",
    "    print(f\"청크 {chunk_number} 처리 시작...\")\n",
    "\n",
    "    # 7.1. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
    "    required_columns = ['id', 'description', 'requirement', 'preferredExperience']\n",
    "    for col in required_columns:\n",
    "        if col not in chunk.columns:\n",
    "            chunk[col] = ''\n",
    "\n",
    "    # 7.2. 'description'과 'requirement'를 합쳐 새로운 열 생성\n",
    "    chunk['description+requirement'] = (\n",
    "        chunk['description'].fillna('') + ' ' +\n",
    "        chunk['requirement'].fillna('')\n",
    "    )\n",
    "\n",
    "    # 7.3. 'preferredExperience'는 별도로 새로운 열 생성\n",
    "    chunk['preferredExperience_cleaned'] = chunk['preferredExperience'].fillna('')\n",
    "\n",
    "    # 7.4. 형태소 분석 적용\n",
    "    chunk['description_requirement_analyzed'] = chunk['description+requirement'].apply(analyze_text_with_custom_words)\n",
    "    chunk['description_requirement_analyzed_all'] = chunk['description+requirement'].apply(analyze_text_all_with_custom_words)\n",
    "    chunk['preferredExperience_analyzed'] = chunk['preferredExperience_cleaned'].apply(analyze_text_with_custom_words)\n",
    "    chunk['preferredExperience_analyzed_all'] = chunk['preferredExperience_cleaned'].apply(analyze_text_all_with_custom_words)\n",
    "\n",
    "    print(\"형태소 분석 완료.\")\n",
    "    print('-'*80)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNRNDz1nu7LsPxz5VGHfErv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
