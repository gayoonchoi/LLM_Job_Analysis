{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jcW8rl5YHr9Z",
        "outputId": "571ba1fd-bbc1-4c1b-b665-a3d8ec29e52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting kiwipiepy_model<0.21,>=0.20 (from kiwipiepy)\n",
            "  Downloading kiwipiepy_model-0.20.0.tar.gz (34.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kiwipiepy_model\n",
            "  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.20.0-py3-none-any.whl size=34818026 sha256=3b3e0370ff2f875b2833188593607093bf0231fa4722196d27381e242a4114b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/c8/52/3a539d6e9065b191fe1c215e0203dcc3e00601c0e3d3d39824\n",
            "Successfully built kiwipiepy_model\n",
            "Installing collected packages: kiwipiepy_model, kiwipiepy\n",
            "Successfully installed kiwipiepy-0.20.3 kiwipiepy_model-0.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kiwipiepy pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "import kiwipiepy"
      ],
      "metadata": {
        "id": "2-66w7LFIdxz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "import os\n",
        "\n",
        "# 1. CSV 파일 경로 설정 (실제 파일 경로로 수정하세요)\n",
        "merged_df_file = '/content/merged_df_2025-01-17-16.csv'          # 입력 CSV 파일 경로\n",
        "user_dictionary_file = '/content/user_dictionary.txt'   # 사용자 사전 파일 경로\n",
        "output_file = '/content/analyzed_job_data_final7.csv'            # 출력 CSV 파일 경로\n",
        "\n",
        "# 2. CSV 파일 읽기\n",
        "try:\n",
        "    merged_df = pd.read_csv(merged_df_file)\n",
        "    print(\"CSV 파일 읽기 완료.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 파일을 찾을 수 없습니다. 경로를 확인하세요: {merged_df_file}\")\n",
        "    exit(1)\n",
        "\n",
        "# 3. Kiwi 초기화 및 사용자 사전 적용\n",
        "kiwi = Kiwi(typos='basic_with_continual_and_lengthening')  # 기본 오타 정보, 연철, 장음화 함께 사용\n",
        "\n",
        "# 3.1. 사용자 사전 파일에서 단어 읽기\n",
        "try:\n",
        "    with open(user_dictionary_file, 'r', encoding='utf-8') as f:\n",
        "        user_custom_words = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"사용자 사전 단어 수: {len(user_custom_words)}개\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 사용자 사전 파일을 찾을 수 없습니다. 경로를 확인하세요: {user_dictionary_file}\")\n",
        "    exit(1)\n",
        "\n",
        "# 3.2. 사용자 사전 단어를 NNP 품사로 등록\n",
        "for word in user_custom_words:\n",
        "    kiwi.add_user_word(word, 'NNP')  # IT 용어는 보통 고유 명사(NNP)로 분류\n",
        "\n",
        "print(\"사용자 사전 단어 등록 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK_NmfL2I4Mw",
        "outputId": "fab9f38d-6b0a-48e3-cdcb-51188be48337"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV 파일 읽기 완료.\n",
            "사용자 사전 단어 수: 832개\n",
            "사용자 사전 단어 등록 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석에서 기능 바꾸기"
      ],
      "metadata": {
        "id": "6716gUxcQPwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 형태소 분석 함수 정의\n",
        "allowed_pos_tags = ['NNG', 'NNP', 'SL']  # 일반 명사, 고유 명사, 외래어\n",
        "\n",
        "def analyze_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True,  split_complex=True)\n",
        "    morphs = [morph for morph, pos, _, _ in tokens if pos in allowed_pos_tags]\n",
        "    # 중복된 토큰 제거 (순서 유지)\n",
        "    seen = set()\n",
        "    unique_morphs = []\n",
        "    for morph in morphs:\n",
        "        if morph not in seen:\n",
        "            seen.add(morph)\n",
        "            unique_morphs.append(morph)\n",
        "    return unique_morphs\n",
        "\n",
        "def analyze_text_all(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True,  split_complex=True)\n",
        "    combined_tokens = []\n",
        "    i = 0\n",
        "    morph_seen = set()\n",
        "    while i < len(tokens):\n",
        "        morph, pos, _, _ = tokens[i]\n",
        "        # 현재 토큰이 SN이고 다음 토큰이 NNB인 경우 결합\n",
        "        if pos == 'SN' and (i + 1) < len(tokens):\n",
        "            next_morph, next_pos, _, _ = tokens[i + 1]\n",
        "            if next_pos == 'NNB':\n",
        "                combined_morph = morph + next_morph\n",
        "                combined_pos = 'SN+NNB'\n",
        "                if combined_morph not in morph_seen:\n",
        "                    combined_tokens.append(f\"{combined_morph}+{combined_pos}\")\n",
        "                    morph_seen.add(combined_morph)\n",
        "                i += 2  # 다음 토큰으로 건너뜀\n",
        "                continue\n",
        "        # 그렇지 않으면 기존 방식대로 처리\n",
        "        if morph not in morph_seen:\n",
        "            combined_tokens.append(f\"{morph}+{pos}\")\n",
        "            morph_seen.add(morph)\n",
        "        i += 1\n",
        "    return combined_tokens\n",
        "\n",
        "# 6. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
        "required_columns = ['description', 'requirement', 'preferredExperience']\n",
        "for col in required_columns:\n",
        "    if col not in merged_df.columns:\n",
        "        merged_df[col] = ''\n",
        "\n",
        "# 7. 형태소 분석 적용\n",
        "print(\"형태소 분석 시작...\")\n",
        "merged_df['description_analyzed'] = merged_df['description'].apply(analyze_text)\n",
        "merged_df['requirement_analyzed'] = merged_df['requirement'].apply(analyze_text)\n",
        "merged_df['preferredExperience_analyzed'] = merged_df['preferredExperience'].apply(analyze_text)\n",
        "\n",
        "# 모든 형태소 및 품사 추출 (선택 사항)\n",
        "merged_df['description_analyzed_all'] = merged_df['description'].apply(analyze_text_all)\n",
        "merged_df['requirement_analyzed_all'] = merged_df['requirement'].apply(analyze_text_all)\n",
        "merged_df['preferredExperience_analyzed_all'] = merged_df['preferredExperience'].apply(analyze_text_all)\n",
        "\n",
        "print(\"형태소 분석 완료.\")\n",
        "print('-'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66lmjCDtJIji",
        "outputId": "5e5369f0-8793-420e-c5db-f35f2e6ee66a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 분석 시작...\n",
            "형태소 분석 완료.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 결과를 새로운 CSV 파일에 저장\n",
        "output_columns = [\n",
        "    'id',\n",
        "    'description',\n",
        "    'requirement',\n",
        "    'preferredExperience',\n",
        "    'description_analyzed',\n",
        "    'requirement_analyzed',\n",
        "    'preferredExperience_analyzed',\n",
        "    'description_analyzed_all',\n",
        "    'requirement_analyzed_all',\n",
        "    'preferredExperience_analyzed_all'\n",
        "]\n",
        "\n",
        "# 필요한 열만 선택하여 저장 (존재하지 않는 열은 제외)\n",
        "existing_output_columns = [col for col in output_columns if col in merged_df.columns]\n",
        "\n",
        "# 출력 디렉토리가 존재하지 않으면 생성\n",
        "output_dir = os.path.dirname(output_file)\n",
        "if output_dir and not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# CSV 파일로 저장\n",
        "merged_df[existing_output_columns].to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "print(f\"형태소 분석 결과가 '{output_file}' 파일에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqmILFTBJ3yA",
        "outputId": "4d257fd9-2026-4d5c-c03a-f140253f8731"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 분석 결과가 '/content/analyzed_job_data_final7.csv' 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# description + requirement 합쳐서 분석 다시 수행"
      ],
      "metadata": {
        "id": "pAx1b9hyOqF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/content/analyzed_job_data_final8.csv'"
      ],
      "metadata": {
        "id": "185bDc3cOtVw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 형태소 분석 함수 정의\n",
        "allowed_pos_tags = ['NNG', 'NNP', 'SL']  # 일반 명사, 고유 명사, 외래어\n",
        "\n",
        "def analyze_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True, split_complex=True)\n",
        "    morphs = [morph for morph, pos, _, _ in tokens if pos in allowed_pos_tags]\n",
        "    seen = set()\n",
        "    unique_morphs = []\n",
        "    for morph in morphs:\n",
        "        if morph not in seen:\n",
        "            seen.add(morph)\n",
        "            unique_morphs.append(morph)\n",
        "    return unique_morphs\n",
        "\n",
        "def analyze_text_all(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True, split_complex=True)\n",
        "    combined_tokens = []\n",
        "    morph_seen = set()\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        morph, pos, _, _ = tokens[i]\n",
        "        if pos == 'SN' and (i + 1) < len(tokens):\n",
        "            next_morph, next_pos, _, _ = tokens[i + 1]\n",
        "            if next_pos == 'NNB':\n",
        "                combined_morph = morph + next_morph\n",
        "                combined_pos = 'SN+NNB'\n",
        "                if combined_morph not in morph_seen:\n",
        "                    combined_tokens.append(f\"{combined_morph}+{combined_pos}\")\n",
        "                    morph_seen.add(combined_morph)\n",
        "                i += 2\n",
        "                continue\n",
        "        if morph not in morph_seen:\n",
        "            combined_tokens.append(f\"{morph}+{pos}\")\n",
        "            morph_seen.add(morph)\n",
        "        i += 1\n",
        "    return combined_tokens\n",
        "\n",
        "# 5. 청크 크기 설정\n",
        "chunksize = 10000  # 필요에 따라 조정\n",
        "\n",
        "# 6. 출력 파일 초기화 (헤더 포함)\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# 7. CSV 파일 청크 단위로 읽기 및 처리\n",
        "for chunk_number, chunk in enumerate(pd.read_csv(merged_df_file, chunksize=chunksize), start=1):\n",
        "    print(f\"청크 {chunk_number} 처리 시작...\")\n",
        "\n",
        "    # 7.1. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
        "    required_columns = ['id', 'description', 'requirement', 'preferredExperience']\n",
        "    for col in required_columns:\n",
        "        if col not in chunk.columns:\n",
        "            chunk[col] = ''\n",
        "\n",
        "    # 7.2. 'description'과 'requirement'를 합쳐 새로운 열 생성\n",
        "    chunk['description+requirement'] = (\n",
        "        chunk['description'].fillna('') + ' ' +\n",
        "        chunk['requirement'].fillna('')\n",
        "    )\n",
        "\n",
        "    # 7.3. 'preferredExperience'는 별도로 새로운 열 생성\n",
        "    chunk['preferredExperience_cleaned'] = chunk['preferredExperience'].fillna('')\n",
        "\n",
        "    # 7.4. 형태소 분석 적용\n",
        "    chunk['description_requirement_analyzed'] = chunk['description+requirement'].apply(analyze_text)\n",
        "    chunk['description_requirement_analyzed_all'] = chunk['description+requirement'].apply(analyze_text_all)\n",
        "    chunk['preferredExperience_analyzed'] = chunk['preferredExperience_cleaned'].apply(analyze_text)\n",
        "    chunk['preferredExperience_analyzed_all'] = chunk['preferredExperience_cleaned'].apply(analyze_text_all)\n",
        "\n",
        "    print(\"형태소 분석 완료.\")\n",
        "    print('-'*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETiJJMZDSS5S",
        "outputId": "59477de1-666a-494e-9f96-42df0eeb1bdc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청크 1 처리 시작...\n",
            "형태소 분석 완료.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 7.5. 결과를 새로운 CSV 파일에 저장\n",
        "output_columns = [\n",
        "        'id',\n",
        "        'description',\n",
        "        'requirement',\n",
        "        'preferredExperience',\n",
        "        'description+requirement',\n",
        "        'description_requirement_analyzed',\n",
        "        'description_requirement_analyzed_all',\n",
        "        'preferredExperience_analyzed',\n",
        "        'preferredExperience_analyzed_all'\n",
        "    ]\n",
        "\n",
        "    # 필요한 열만 선택하여 저장 (존재하지 않는 열은 제외)\n",
        "existing_output_columns = [col for col in output_columns if col in chunk.columns]\n",
        "\n",
        "    # 결과를 CSV 파일에 저장 (첫 청크는 헤더 포함, 이후 청크는 헤더 제외)\n",
        "chunk[existing_output_columns].to_csv(\n",
        "        output_file,\n",
        "        mode='a',\n",
        "        index=False,\n",
        "        encoding='utf-8-sig',\n",
        "        header=(chunk_number == 1)  # 첫 청크에만 헤더 포함\n",
        "    )\n",
        "\n",
        "print(f\"청크 {chunk_number} 처리 완료. 현재까지 {chunk_number * chunksize}개의 행 처리됨.\")\n",
        "print('-'*80)\n",
        "\n",
        "print(f\"모든 청크의 형태소 분석 결과가 '{output_file}' 파일에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKB9f85yS1x7",
        "outputId": "76c2071a-a6b1-4300-e9a2-ede2089ba088"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청크 1 처리 완료. 현재까지 10000개의 행 처리됨.\n",
            "--------------------------------------------------------------------------------\n",
            "모든 청크의 형태소 분석 결과가 '/content/analyzed_job_data_final8.csv' 파일에 저장되었습니다.\n"
          ]
        }
      ]
    }
  ]
}