{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BD6mAxOQWdQN",
        "outputId": "4d63b869-38d6-430c-8fa2-846e4f64325b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting kiwipiepy_model<0.21,>=0.20 (from kiwipiepy)\n",
            "  Downloading kiwipiepy_model-0.20.0.tar.gz (34.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kiwipiepy) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading kiwipiepy-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kiwipiepy_model\n",
            "  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.20.0-py3-none-any.whl size=34818026 sha256=218817eb3878924669acc91d7088ddd73cd1f6e9c938967564ba0239be81d0f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/c8/52/3a539d6e9065b191fe1c215e0203dcc3e00601c0e3d3d39824\n",
            "Successfully built kiwipiepy_model\n",
            "Installing collected packages: kiwipiepy_model, kiwipiepy\n",
            "Successfully installed kiwipiepy-0.20.3 kiwipiepy_model-0.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kiwipiepy pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "import kiwipiepy\n",
        "import os"
      ],
      "metadata": {
        "id": "RGyymIQyWq7K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 형태소 분석하기"
      ],
      "metadata": {
        "id": "WoAp53s9eL_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from kiwipiepy import Kiwi\n",
        "import os\n",
        "\n",
        "# 1. CSV 파일 경로 설정 (실제 파일 경로로 수정하세요)\n",
        "merged_df_file = '/content/2025-01-21-12_final_lower.csv'          # 입력 CSV 파일 경로\n",
        "user_dictionary_file = '/content/technicalTags (3).txt'   # 사용자 사전 파일 경로\n",
        "output_file = '/content/[lower]analyzed_job_data_final.csv'            # 출력 CSV 파일 경로\n",
        "\n",
        "# 2. CSV 파일 읽기\n",
        "try:\n",
        "    merged_df = pd.read_csv(merged_df_file)\n",
        "    print(\"CSV 파일 읽기 완료.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 파일을 찾을 수 없습니다. 경로를 확인하세요: {merged_df_file}\")\n",
        "    exit(1)\n",
        "\n",
        "# 3. Kiwi 초기화 및 사용자 사전 적용\n",
        "# kiwi = Kiwi(typos='basic_with_continual_and_lengthening')  # 기본 오타 정보, 연철, 장음화 함께 사용\n",
        "kiwi = Kiwi(typos='basic_with_continual')\n",
        "\n",
        "# 3.1. 사용자 사전 파일에서 단어 읽기\n",
        "try:\n",
        "    with open(user_dictionary_file, 'r', encoding='utf-8') as f:\n",
        "        user_custom_words = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"사용자 사전 단어 수: {len(user_custom_words)}개\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"오류: 사용자 사전 파일을 찾을 수 없습니다. 경로를 확인하세요: {user_dictionary_file}\")\n",
        "    exit(1)\n",
        "\n",
        "# 3.2. 사용자 사전 단어를 NNP 품사로 등록\n",
        "for word in user_custom_words:\n",
        "    kiwi.add_user_word(word, 'SL')  # IT 용어는 보통 고유 명사(NNP)로 분류\n",
        "\n",
        "print(\"사용자 사전 단어 등록 완료.\")"
      ],
      "metadata": {
        "id": "SFp6MVYseEiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. 형태소 분석 함수 정의\n",
        "allowed_pos_tags = ['SL']  # 외래어만 추출\n",
        "\n",
        "def analyze_foreign_words(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True, split_complex=True)\n",
        "    # 외래어(SL)만 추출\n",
        "    foreign_words = [morph for morph, pos, _, _ in tokens if pos == 'SL']\n",
        "    # 중복된 토큰 제거 (순서 유지)\n",
        "    seen = set()\n",
        "    unique_foreign_words = []\n",
        "    for morph in foreign_words:\n",
        "        if morph not in seen:\n",
        "            seen.add(morph)\n",
        "            unique_foreign_words.append(morph)\n",
        "    return unique_foreign_words\n",
        "\n",
        "def analyze_foreign_words_all(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    # 형태소 분석 수행 (불용어 제거 및 IT 용어 고정)\n",
        "    tokens = kiwi.tokenize(text, normalize_coda=True,  split_complex=True)\n",
        "    combined_tokens = []\n",
        "    morph_seen = set()\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        morph, pos, _, _ = tokens[i]\n",
        "        # 현재 토큰이 SN이고 다음 토큰이 NNB인 경우 결합 (외래어 포함)\n",
        "        if pos == 'SN' and (i + 1) < len(tokens):\n",
        "            next_morph, next_pos, _, _ = tokens[i + 1]\n",
        "            if next_pos == 'NNB':\n",
        "                combined_morph = morph + next_morph\n",
        "                combined_pos = 'SN+NNB'\n",
        "                if pos == 'SL' and combined_morph not in morph_seen:\n",
        "                    combined_tokens.append(f\"{combined_morph}+{combined_pos}\")\n",
        "                    morph_seen.add(combined_morph)\n",
        "                i += 2\n",
        "                continue\n",
        "        # 외래어(SL)만 처리\n",
        "        if pos == 'SL' and morph not in morph_seen:\n",
        "            combined_tokens.append(f\"{morph}+{pos}\")\n",
        "            morph_seen.add(morph)\n",
        "        i += 1\n",
        "    return combined_tokens\n"
      ],
      "metadata": {
        "id": "H0oloTEo0a8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
        "required_columns = ['description', 'requirement', 'preferredExperience']\n",
        "for col in required_columns:\n",
        "    if col not in merged_df.columns:\n",
        "        merged_df[col] = ''\n",
        "\n",
        "# 7. 'description'과 'requirement'를 합쳐 새로운 열 생성\n",
        "merged_df['description_requirement'] = (\n",
        "    merged_df['description'].fillna('') + ' ' +\n",
        "    merged_df['requirement'].fillna('')\n",
        ")\n",
        "\n",
        "# 8. 'preferredExperience'는 별도로 새로운 열 생성 (필요시 다른 전처리 추가 가능)\n",
        "merged_df['preferredExperience_cleaned'] = merged_df['preferredExperience'].fillna('')\n",
        "\n",
        "# 9. 형태소 분석 적용\n",
        "print(\"형태소 분석 시작...\")\n",
        "merged_df['combined_analyzed'] = merged_df['description_requirement'].apply(analyze_text)\n",
        "merged_df['combined_analyzed_all'] = merged_df['description_requirement'].apply(analyze_text_all)\n",
        "merged_df['preferredExperience_analyzed'] = merged_df['preferredExperience_cleaned'].apply(analyze_text)\n",
        "merged_df['preferredExperience_analyzed_all'] = merged_df['preferredExperience_cleaned'].apply(analyze_text_all)\n",
        "print(\"형태소 분석 완료.\")\n",
        "print('-'*80)"
      ],
      "metadata": {
        "id": "TiJu1vmX1D-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.5. 결과를 새로운 CSV 파일에 저장\n",
        "output_columns = [\n",
        "    'id',\n",
        "    'description',\n",
        "    'requirement',\n",
        "    'preferredExperience',\n",
        "    'description_requirement',  # 'description+requirement' -> 'description_requirement'으로 변경\n",
        "    'combined_analyzed',  # 'description_requirement_analyzed' -> 'combined_analyzed'으로 변경\n",
        "    'combined_analyzed_all',  # 'description_requirement_analyzed_all' -> 'combined_analyzed_all'으로 변경\n",
        "    'preferredExperience_analyzed',\n",
        "    'preferredExperience_analyzed_all'\n",
        "]\n",
        "# 필요한 열만 선택하여 저장 (존재하지 않는 열은 제외)\n",
        "existing_output_columns = [col for col in output_columns if col in merged_df.columns]\n",
        "\n",
        "# 출력 디렉토리가 존재하지 않으면 생성\n",
        "output_dir = os.path.dirname(output_file)\n",
        "if output_dir and not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# CSV 파일로 저장\n",
        "merged_df[existing_output_columns].to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "print(f\"형태소 분석 결과가 '{output_file}' 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "id": "0MBbJMjx0iAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. 청크 크기 설정\n",
        "chunksize = 10000  # 필요에 따라 조정\n",
        "\n",
        "# 6. 출력 파일 초기화 (헤더 포함)\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# 7. CSV 파일 청크 단위로 읽기 및 처리\n",
        "for chunk_number, chunk in enumerate(pd.read_csv(merged_df_file, chunksize=chunksize), start=1):\n",
        "    print(f\"청크 {chunk_number} 처리 시작...\")\n",
        "\n",
        "    # 7.1. 필요한 열이 존재하는지 확인하고, 없으면 빈 문자열로 채움\n",
        "    required_columns = ['id', 'description', 'requirement', 'preferredExperience']\n",
        "    for col in required_columns:\n",
        "        if col not in chunk.columns:\n",
        "            chunk[col] = ''\n",
        "\n",
        "    # 7.2. 'description'과 'requirement'를 합쳐 새로운 열 생성\n",
        "    chunk['description+requirement'] = (\n",
        "        chunk['description'].fillna('') + ' ' +\n",
        "        chunk['requirement'].fillna('')\n",
        "    )\n",
        "\n",
        "    # 7.3. 'preferredExperience'는 별도로 새로운 열 생성\n",
        "    chunk['preferredExperience_cleaned'] = chunk['preferredExperience'].fillna('')\n",
        "\n",
        "    # 7.4. 형태소 분석 적용\n",
        "    chunk['description_requirement_analyzed'] = chunk['description+requirement'].apply(analyze_text_with_custom_words)\n",
        "    chunk['description_requirement_analyzed_all'] = chunk['description+requirement'].apply(analyze_text_all_with_custom_words)\n",
        "    chunk['preferredExperience_analyzed'] = chunk['preferredExperience_cleaned'].apply(analyze_text_with_custom_words)\n",
        "    chunk['preferredExperience_analyzed_all'] = chunk['preferredExperience_cleaned'].apply(analyze_text_all_with_custom_words)\n",
        "\n",
        "    print(\"형태소 분석 완료.\")\n",
        "    print('-'*80)\n"
      ],
      "metadata": {
        "id": "LUXy3lvr0bgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}